{
    "contents" : "---\ntitle: \"Report on the NLP models\"\nauthor: \"Miao YU\"\ndate: \"2014/9/24\"\noutput: html_document\n---\n\n# Introduction\n\nPortable office actually means the works done on the cellphones or tablets and we need input system to saving our time on typing on those device. So a smart and efficient keyboard is required. The core of this input system is a predictive text model. This report is focused on this model, covering the very beginning, namely data collection, to the final products on the shinyapp.io . The first a few parts came from the milestone report with a revision.\n\n# Data Collection\n\nThe data were downloaded from the course website (from [HC Corpora](www.corpora.heliohost.org)) and unzipped to extract the English database as a corpus. Three text documents from the twitter, blog and news were found with each line standing for a documenet.\n\n# Data Pre-Summary\n\n# Data Pre-Summary\n\nAfter scan the three documents with `bash`, I found the following features:\n\n- the basic summary of the data set is shown as follows:\n\n```{r axis = T,echo = F,cache=TRUE}\nlibrary(knitr)\ntwitter <- system('wc -lwm data/final/en_US/en_US.twitter.txt',intern = T)\nnews <- system('wc -lwm data/final/en_US/en_US.news.txt',intern = T)\nblogs <- system('wc -lwm data/final/en_US/en_US.blogs.txt',intern = T)\nten <- as.numeric(grep('[[:digit:]]', unlist(strsplit(twitter,\" \")), value = T))\nnen <- as.numeric(grep('[[:digit:]]', unlist(strsplit(news,\" \")), value = T))\nben <- as.numeric(grep('[[:digit:]]', unlist(strsplit(blogs,\" \")), value = T))\nen <- as.data.frame(rbind(ten,nen,ben))\nrownames(en) <- c('twitter','news','blogs')\ncolnames(en) <- c('line counts','word counts','document size')\nkable(en, align='c', caption = \"Summary of the datasets\")\n```\n\n- twitter is short(of course less than 140) with a lot of informal characters and less grammar, which means more noise\n- news is written in a formal manner but the topics is focused\n- blog's style is between the twitter and news with less noise and more topics\n- the average length of each lines in the three database: blog > news > twitter, which means blog is the longest document class and longer document will help to build a better model for prediction in certain context\n\nSo, the blog data will be good for us to build a model if those three document is too large to be loaded for exploring. However, using sampling will ease the burden on the calculation and finally I sampled 30,000 20,000 and 10,000 lines with seed from the blogs, news and twitter database for exploring and training a model and the left data will be sampled to make the test data sets.\n\n```{r echo = F,cache=TRUE,warning=FALSE}\nlibrary(tm)\nlibrary(stringi)\nent <- readLines('data/final/en_US/en_US.twitter.txt', encoding = 'UTF-8')\nenn <- readLines('data/final/en_US/en_US.news.txt', encoding = 'UTF-8')\nenb <- readLines('data/final/en_US/en_US.blogs.txt', encoding = 'UTF-8')\nset.seed(1)\nsubent <- ent[sample(1:length(ent),10000)]\nset.seed(1)\nsubenn <- enn[sample(1:length(enn),20000)]\nset.seed(1)\nsubenb <- enb[sample(1:length(enb),30000)]\nsuben <- c(subent,subenn,subenb)\nrm(enb,enn,ent,subenb,subenn,subent)\n```\n\n# Tokenization\n\nThe whole tokenization is aiming at removing meaningless characters and the words with low frequency in the corpus. The final corpus will show the words  or n-gram with a high frequency which will be helpful for exploring the relationship between the words and building a manful statistical model.\n\nSo, I extracted 1)the ASCII characters, 2)change the capital characters to lower case, 3)remove the punctuation, 4)numbers and 5)stop words and 6)stemming the left words. To decrease the spares of the term frequency, I removed the terms occurred less than ten times in the whole document to get the final corpus. \n\n```{r echo=F,cache=TRUE}\nascllen <- stri_enc_toascii(suben)\nascllen <- stri_replace_all_regex(ascllen,'\\032','')\nen <- Corpus(VectorSource(ascllen))\n\nenall <- tm_map(en, content_transformer(tolower))\nenall <- tm_map(enall, removePunctuation)\nenall <- tm_map(enall, removeNumbers)\nenall <- tm_map(enall, removeWords, stopwords(\"english\"))\nenall <- tm_map(enall, stemDocument,language = (\"english\"))\nenall <- tm_map(enall, stripWhitespace)\n\n# url <- 'http://www-personal.umich.edu/~jlawler/wordlist'\n# dic <- download.file(url,'data/dic.txt', method = 'curl')\n# dic <- readLines('data/dic.txt', encoding = 'UTF-8')\n\nctrl <- list(tokenize = words, bounds = list(global = c(10,Inf)))\n\noptions(mc.cores=1)\n\nBigramTokenizer <- function(x) {RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 2))}\nctrl2 <- list(tokenize = BigramTokenizer, bounds = list(global = c(10,Inf)))\n\nTrigramTokenizer <- function(x) {RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 3, max = 3))}\nctrl3 <- list(tokenize = TrigramTokenizer, bounds = list(global = c(10,Inf)))\n\n# TeragramTokenizer <- function(x) {RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 4, max = 4))}\n# ctrl4 <- list(tokenize = TeragramTokenizer, bounds = list(global = c(10,Inf)))\n\nTokenizer <- function(x) {RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 1, max = 3))}\nctrl0 <- list(tokenize = Tokenizer, bounds = list(global = c(10,Inf)))\n\nlibrary(slam)\nen.tdm <- TermDocumentMatrix(enall,control = ctrl)\nen.bitdm <- TermDocumentMatrix(enall,control = ctrl2)\nen.tritdm <- TermDocumentMatrix(enall,control = ctrl3)\n# en.teratdm <- TermDocumentMatrix(enall,control = ctrl4)\nen.tdm0 <- TermDocumentMatrix(enall,control = ctrl0)\n\nfreq <- rowapply_simple_triplet_matrix(en.tdm,sum)\nfreqbi <- rowapply_simple_triplet_matrix(en.bitdm,sum)\nfreqtri <- rowapply_simple_triplet_matrix(en.tritdm,sum)\n# freqtera <- rowapply_simple_triplet_matrix(en.teratdm,sum)\nfreq0 <- rowapply_simple_triplet_matrix(en.tdm0,sum)\n```\n\n# Exploratory analysis\n\nTo build a n-gram model, I extracted n-gram corpus with the help of `RWeka` package. The uni gram terms corpus has `r length(en.tdm$dimnames$Terms)` words, the bi gram corpus has `r length(en.bitdm$dimnames$Terms)` terms and the tri gram corpus has `r length(en.tritdm$dimnames$Terms)` terms. Then I explored three corpus(uni gram, bi gram and tri gram) and made a histogram to show the distribution of the terms in them. \n\n```{r,cache=TRUE,echo=FALSE,warning=FALSE,message=FALSE}\npar(oma=c(0,0,3,0),mfrow = c(2,2), mar=c(2,2,2,2))\nhist(log(freq), breaks = 50, main = 'uni gram corpus', xlab='the log value of the Frequency', ylab='')\nhist(log(freqbi), breaks = 50, main = 'bi gram corpus', xlab='the log value of the Frequency', ylab='')\nhist(log(freqtri), breaks = 50, main = 'tri gram corpus', xlab='the log value of the Frequency', ylab='')\nlibrary(wordcloud)\nwordcloud(names(freq0), freq0, min.freq = 400)\ntitle(\"Figure 1: Histogram of term frequency and word cloud of all of the three corpus\",outer=T)\n```\n\nAs shown in Figure 1, the logged frequencies in all of the three corpus were still skewed to the left, which mean the sparse of the terms data. So I think it will be hard to build a good generation regression model but local regression would be OK. Also I found only 8063 words occurred more than ten times in the sampled documents compared with nearly 70 thousand words in an online [dictionary](http://www-personal.umich.edu/~jlawler/wordlist), which mean focused on little words would work in most of the prediction. The word cloud showed the terms occurred more than 400 and those terms would be good to build a classification filter models before using a n-gram model to speed up the whole prediction.\n\nFrom the exploratory analysis I summaried the following items to build the model:\n\n- using current laptop to process the whole dataset will be time-consuming\n- each dataset will be overfitted on the most common words while underfitted on the least common words\n- a pre-classification of the words before will be helpful to narrow down the scales\n- a n-gram model will solve most of the problems in these cases\n- using different sources of documents will be helpful for a supervised learning while a PCA analysis of the documents will also be helpful for a unsupervised learning to group the source\n\n\n# Modeling\n\nThe modeling process will be shown as follows. Firstly, I sampled the origin train data to get a corpus without capital characters, punctuation, number and stop words. The most freqency words were used to classify the original data into different sources. Here I use news, blogs and twitters as three different sources. However, only three sources made the model building very slow. I thought maybe I could only use the most frequency words. 34 words occured more than 1000 times in 30000 samples were used to class the sentences by a support vector machines. Results were bad and it seemed all of the sentences will be grouped into the twitter group. I dropped this idea in this project but I think classification will be helpful to get many sub-models to speed up the whole models. Maybe some unsupervised learning models will be useful to get some topic groups for the classification.\n\n```{r}\nlibrary(tm)\nlibrary(stringi)\nlibrary(slam)\nent <- readLines('data/final/en_US/en_US.twitter.txt', encoding = 'UTF-8')\nenn <- readLines('data/final/en_US/en_US.news.txt', encoding = 'UTF-8')\nenb <- readLines('data/final/en_US/en_US.blogs.txt', encoding = 'UTF-8')\nset.seed(1)\nindex <- sample(1:length(ent),20000)\nsubenttr <- ent[index]\nsubentts <- ent[-index]\nsubentts <- subentts[index]\nset.seed(1)\nindex <- sample(1:length(enn),40000)\nsubenntr <- enn[index]\nsubennts <- enn[-index]\nsubennts <- subennts[index]\nset.seed(1)\nindex <- sample(1:length(enb),60000)\nsubenbtr <- enb[index]\nsubenbts <- enb[-index]\nsubenbts <- subenbts[index]\n\nascllenttr <- stri_enc_toascii(subenttr)\nascllenttr <- stri_replace_all_regex(ascllenttr,'\\032','')\nascllentts <- stri_enc_toascii(subentts)\nascllentts <- stri_replace_all_regex(ascllentts,'\\032','')\n\nascllenntr <- stri_enc_toascii(subenntr)\nascllenntr <- stri_replace_all_regex(ascllenntr,'\\032','')\nascllennts <- stri_enc_toascii(subennts)\nascllennts <- stri_replace_all_regex(ascllennts,'\\032','')\n\nascllenbtr <- stri_enc_toascii(subenbtr)\nascllenbtr <- stri_replace_all_regex(ascllenbtr,'\\032','')\nascllenbts <- stri_enc_toascii(subenbts)\nascllenbts <- stri_replace_all_regex(ascllenbts,'\\032','')\n\ntest <- VectorSource(c(ascllenttr,ascllenntr,ascllenbtr))\nen <- Corpus(test)\n\nenall <- tm_map(en, content_transformer(tolower))\nenall <- tm_map(enall, removePunctuation)\nenall <- tm_map(enall, removeNumbers)\nenall <- tm_map(enall, removeWords, stopwords(\"english\"))\nenall <- tm_map(enall, stemDocument,language = (\"english\"))\nenall <- tm_map(enall, stripWhitespace)\n\nctrl <- list(tokenize = words, bounds = list(global = c(1000,Inf)))\nenall.dtm <- DocumentTermMatrix(enall, control = ctrl)\nclassvec <- factor(c(rep('twitter',10000),rep('news',10000),rep('blogs',10000)))\n\nlibrary(Matrix)\nlibrary(SparseM)\nlibrary(e1071)\n\nsvm_model <- svm(enall.dtm, classvec)\nclasspre <- predict(svm_model,enall.dtm)\ntable(classpre,classvec)\n```\n\nOK,it seemed I need to train all of the three kinds of corpus to get a n-gram model. Here I use a trick: if the sentence has more than 139 words, I will use the model without training from twitter group. Then I will use a n-gram model for the prediction. At first I try a smooth method, then I found it will really take a long time to get the n-gram matrix. Here I prefer data to smooth for a better prediction. I increased the data from 100000 samples to 300000 samples while limited the corpus by a least occurances of 6, which meaned I just ignored the n-grams with low freqency and no more smooth needed. So the final model is actully the most simplest n-gram model: when the counts in n-gram is less than 6, a (n-1)-gram model will be used.\n\n```{r}\n# library(tm)\n# library(stringi)\n# library(slam)\n# ent <- readLines('data/final/en_US/en_US.twitter.txt', encoding = 'UTF-8')\n# enn <- readLines('data/final/en_US/en_US.news.txt', encoding = 'UTF-8')\n# enb <- readLines('data/final/en_US/en_US.blogs.txt', encoding = 'UTF-8')\n# set.seed(1)\n# index <- sample(1:length(ent),100000)\n# subenttr <- ent[index]\n# subentts <- ent[-index]\n# subentts <- subentts[index]\n# set.seed(1)\n# index <- sample(1:length(enn),200000)\n# subenntr <- enn[index]\n# subennts <- enn[-index]\n# subennts <- subennts[index]\n# set.seed(1)\n# index <- sample(1:length(enb),300000)\n# subenbtr <- enb[index]\n# subenbts <- enb[-index]\n# subenbts <- subenbts[index]\n# \n# ascllenttr <- stri_enc_toascii(subenttr)\n# ascllenttr <- stri_replace_all_regex(ascllenttr,'\\032','')\n# ascllentts <- stri_enc_toascii(subentts)\n# ascllentts <- stri_replace_all_regex(ascllentts,'\\032','')\n# \n# ascllenntr <- stri_enc_toascii(subenntr)\n# ascllenntr <- stri_replace_all_regex(ascllenntr,'\\032','')\n# ascllennts <- stri_enc_toascii(subennts)\n# ascllennts <- stri_replace_all_regex(ascllennts,'\\032','')\n# \n# ascllenbtr <- stri_enc_toascii(subenbtr)\n# ascllenbtr <- stri_replace_all_regex(ascllenbtr,'\\032','')\n# ascllenbts <- stri_enc_toascii(subenbts)\n# ascllenbts <- stri_replace_all_regex(ascllenbts,'\\032','')\n# \n# test <- VectorSource(c(ascllenttr,ascllenntr,ascllenbtr))\n# en <- Corpus(test)\n# \n# save(en,file='data/en.RData')\n# load('data/en.RData')\n# \n# enall <- tm_map(en, content_transformer(tolower))\n# enall <- tm_map(enall, removePunctuation)\n# enall <- tm_map(enall, removeNumbers)\n# enall <- tm_map(enall, removeWords, stopwords(\"english\"))\n# # enall <- tm_map(enall, stemDocument,language = (\"english\"))\n# enall <- tm_map(enall, stripWhitespace)\n# \n# ctrl <- list(tokenize = words, bounds = list(global = c(6,Inf)))\n# options(mc.cores=1)\n# BigramTokenizer <- function(x) {RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 2))}\n# ctrl2 <- list(tokenize = BigramTokenizer, bounds = list(global = c(6,Inf)))\n# TrigramTokenizer <- function(x) {RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 3, max = 3))}\n# ctrl3 <- list(tokenize = TrigramTokenizer, bounds = list(global = c(6,Inf)))\n# en.tdm <- TermDocumentMatrix(enall,control = ctrl)\n# en.bitdm <- TermDocumentMatrix(enall,control = ctrl2)\n# en.tritdm <- TermDocumentMatrix(enall,control = ctrl3)\n# library(slam)\n# freq <- rowapply_simple_triplet_matrix(en.tdm,sum)\n# freqbi <- rowapply_simple_triplet_matrix(en.bitdm,sum)\n# freqtri <- rowapply_simple_triplet_matrix(en.tritdm,sum)\n# firstname <- sapply(strsplit(names(freqbi), ' '), function(a) a[1])\n# secname <- sapply(strsplit(names(freqbi), ' '), function(a) a[2])\n# firsttriname <- sapply(strsplit(names(freqtri), ' '),function(a) a[1])\n# sectriname <- sapply(strsplit(names(freqtri), ' '),function(a) a[2])\n# tritriname <- sapply(strsplit(names(freqtri), ' '),function(a) a[3])\n# length(words1 <- unique(names(freq)))\n# length(words2 <- unique(c(secname,firstname)))\n# length(words3 <- unique(c(tritriname,sectriname,firsttriname)))\n# length(finalwords3 <- intersect(intersect(words1,words2),words3))\n# length(finalwords2 <- intersect(words1,words2))\n# unigramDF <- data.frame(names(freq),freq,stringsAsFactors = F)\n# bigramDF <- data.frame(names(freqbi),freqbi,firstname,secname,stringsAsFactors = F)\n# trigramDF <- data.frame(names(freqtri),freqtri,paste(firsttriname,sectriname),tritriname,stringsAsFactors = F)\n# names(unigramDF) <- c('unigram','freq')\n# names(bigramDF) <- c('bigram','freq','unigram','name')\n# names(trigramDF) <- c('trigram','freq','bigram','name')\n# save(unigramDF,bigramDF,trigramDF,file = 'data/ngram.RData')\nload('data/ngram.RData')\nsource('models.R')\npredict0('I go home',unigramDF,bigramDF,trigramDF)\npredictGT('I go home',unigramDF,bigramDF,trigramDF)\n```\n\n## Estimate D from Good-Turing Estimate\n\n```{r}\nuni.freqfreq <- data.frame(uni=table(unigramDF$freq))\nbi.freqfreq <- data.frame(Bi=table(bigramDF$freq))\ntri.freqfreq <- data.frame(Tri=table(trigramDF$freq))\n        \n# GTCount is used for Good Turing Discounting\n        \nGTCount <- as.data.frame(matrix(rep(NA,28),nrow = 7))\nGTCount[1:31,1] <- c(0:30)\nGTCount[1,2] <- length(unigramDF$unigram)\nGTCount[1,3] <- length(bigramDF$bigram)\nGTCount[1,4] <- length(trigramDF$trigram)\n        \nGTCount[2:31,2] <- uni.freqfreq[1:30,2]\nGTCount[2:31,3] <- bi.freqfreq[1:30,2]\nGTCount[2:31,4] <- tri.freqfreq[1:30,2]\n\nfor (c in 0:29) GTCount[c+2,5]<-(c+1)*GTCount[c+2,2]/GTCount[c+1,2]\nfor (c in 0:29) GTCount[c+2,6]<-(c+1)*GTCount[c+2,3]/GTCount[c+1,3]\nfor (c in 0:29) GTCount[c+2,7]<-(c+1)*GTCount[c+2,4]/GTCount[c+1,4]\n\nplot(GTCount[,1]-GTCount[,5])\nplot(GTCount[,1]-GTCount[,6])\nplot(GTCount[,1]-GTCount[,7])\n\n# D is not fixed\n\n# Get D by Ney's estimate\n\nD <- GTCount[2,2]/(GTCount[2,2]+2*GTCount[3,2])\nD1 <- 1-2*D*GTCount[3,2]/GTCount[2,2]\nD2 <- 2-3*D*GTCount[4,2]/GTCount[3,2]\nD3 <- 3-4*D*GTCount[5,2]/GTCount[4,2]\n\ncon <- unique(bigramDF$name)\nconuni <- rep(NA,length(con))\nfor(i in 1:length(con)) {\n        temp <- con[i]\n        sum <- grepl(paste0(temp,\"$\"),bigramDF$bigram)\n        subbi <- bigramDF[sum,]\n        conuni[i] <- sum(sum)/sum(subbi$freq)\n        }\n```\n\n",
    "created" : 1412907250707.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3677722015",
    "id" : "F5B31ED8",
    "lastKnownWriteTime" : 1412784788,
    "path" : "~/Capstone/finalreport.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}